{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import the pandas package, then use the \"read_csv\" function to read\n",
    "# the labeled training data\n",
    "import pandas as pd       \n",
    "train = pd.read_csv(\"ICHI-corpus3-MOD.tsv\", header=0, \\\n",
    "                    delimiter=\"\\t\", quoting=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read 105  train and test \n"
     ]
    }
   ],
   "source": [
    "# Verify the number of reviews that were read (100,000 in total)\n",
    "print \"Read %d  train and test \" % (train[\"question\"].size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sugar free My 90 year old Daddy just got diagnosed, the one thing he loves is Ice cream, can he eat sugar free ice cream?\n"
     ]
    }
   ],
   "source": [
    "print train[\"question\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11\n"
     ]
    }
   ],
   "source": [
    "print (train[\"id\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup             \n",
    "import re\n",
    "import nltk\n",
    "#nltk.download()  # Download text data sets, including stop words\n",
    "from nltk.corpus import stopwords # Import the stop word list\n",
    "#print (stopwords.words(\"english\") )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def question_to_words( raw_review ):\n",
    "    # Function to convert a raw review to a string of words\n",
    "    # The input is a single string (a raw movie review), and \n",
    "    # the output is a single string (a preprocessed movie review)\n",
    "    #\n",
    "    # 1. Remove HTML\n",
    "    review_text = BeautifulSoup(raw_review, \"html.parser\").get_text() \n",
    "    #\n",
    "    # 2. Remove non-letters        \n",
    "    letters_only = re.sub(\"[^a-zA-Z]\", \" \", review_text) \n",
    "    #\n",
    "    # 3. Convert to lower case, split into individual words\n",
    "    words = letters_only.lower().split()                             \n",
    "    #\n",
    "    # 4. In Python, searching a set is much faster than searching\n",
    "    #   a list, so convert the stop words to a set\n",
    "    stops = set(stopwords.words(\"english\"))                  \n",
    "    # \n",
    "    # 5. Remove stop words\n",
    "    meaningful_words = [w for w in words if not w in stops]   \n",
    "    #\n",
    "    # 6. Join the words back into one string separated by space, \n",
    "    # and return the result.\n",
    "    return( \" \".join( meaningful_words )) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "105\n"
     ]
    }
   ],
   "source": [
    "# Get the number of reviews based on the dataframe column size\n",
    "num_questions_train = train[\"question\"].size\n",
    "print num_questions_train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning and parsing  questions...\n",
      "\n",
      "Question from train 10 of 105\n",
      "\n",
      "Question from train 20 of 105\n",
      "\n",
      "Question from train 30 of 105\n",
      "\n",
      "Question from train 40 of 105\n",
      "\n",
      "Question from train 50 of 105\n",
      "\n",
      "Question from train 60 of 105\n",
      "\n",
      "Question from train 70 of 105\n",
      "\n",
      "Question from train 80 of 105\n",
      "\n",
      "Question from train 90 of 105\n",
      "\n",
      "Question from train 100 of 105\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print (\"Cleaning and parsing  questions...\\n\")\n",
    "clean_train_questions = []\n",
    "num_questions_train = train[\"question\"].size\n",
    "\n",
    "for i in range( 0, num_questions_train ):\n",
    "    # If the index is evenly divisible by 1000, print a message\n",
    "    if( (i+1)%10 == 0 ):\n",
    "        print (\"Question from train %d of %d\\n\" % ( i+1, num_questions_train )  )                                                                  \n",
    "    clean_train_questions.append( question_to_words( train[\"question\"][i] ))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first  clean question is \n",
      "sugar free year old daddy got diagnosed one thing loves ice cream eat sugar free ice cream\n"
     ]
    }
   ],
   "source": [
    "print \"The first  clean question is \\n\",(clean_train_questions[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda2\\lib\\site-packages\\gensim\\utils.py:855: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "\n",
    "# Load pretrained model (since intermediate data is not included, the model cannot be refined with additional data)\n",
    "model = KeyedVectors.load_word2vec_format('c:/users/gachet/downloads/GoogleNews-vectors-negative300.bin', binary=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'pioglitazone', 0.7217838168144226),\n",
       " (u'statin', 0.7187392711639404),\n",
       " (u'statins', 0.7124181985855103),\n",
       " (u'sitagliptin', 0.7081882953643799),\n",
       " (u'allopurinol', 0.7003565430641174),\n",
       " (u'insulin_glargine', 0.6983603239059448),\n",
       " (u'Metformin', 0.6977676153182983),\n",
       " (u'glimepiride', 0.6926617622375488),\n",
       " (u'methotrexate', 0.6917198896408081),\n",
       " (u'sulfonylurea', 0.6905825734138489)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#test the word2vect pretrained model\n",
    "model.most_similar(\"metformin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    " def expand( word, items):\n",
    "        \"\"\" expand the word using the word2vec model \"\"\"\n",
    "\n",
    "        try:\n",
    "            result = model.most_similar(positive=[word], negative=[], topn=items)\n",
    "        except:\n",
    "            result = []\n",
    "        return [pair[0] for pair in result]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'refined_sugar', u'cane_sugar', u'turbinado']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#test exapnd() function\n",
    "expand(\"sugar\",3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# expand first 10 text (test questions) with 3 terms\n",
    "lista = []\n",
    "for i in range( 0, 10 ):\n",
    "    # If the index is evenly divisible by 1000, print a message\n",
    "    frase = \"\"\n",
    "    for palabra in clean_train_questions[i].split():\n",
    "        frase += palabra\n",
    "        lista_p = expand(palabra,3)\n",
    "        for p in lista_p:\n",
    "            frase += \" \"\n",
    "            frase += p\n",
    "        frase += \" \"\n",
    "    lista.append(frase)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sugar refined_sugar cane_sugar turbinado free Free Six_Flags_website_http://www.sixflags.com FREE year month week months old yearold boy 0ld daddy momma mama dad got get gotten getting diagnosed Diagnosed misdiagnosed rediagnosed one only two three thing things something stuff loves adores hates likes ice Ice Francies_tossed Melting_polar cream crème creams Maximum_Moisture eat eating ate eaten sugar refined_sugar cane_sugar turbinado free Free Six_Flags_website_http://www.sixflags.com FREE ice Ice Francies_tossed Melting_polar cream crème creams Maximum_Moisture \n"
     ]
    }
   ],
   "source": [
    "#test first question expanded \n",
    "print lista[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#replace the original 10 first question with expanded ones\n",
    "for i in range( 0, 10 ):\n",
    "   \n",
    "    clean_train_questions[i] = lista[i]\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sugar refined_sugar cane_sugar turbinado free Free Six_Flags_website_http://www.sixflags.com FREE year month week months old yearold boy 0ld daddy momma mama dad got get gotten getting diagnosed Diagnosed misdiagnosed rediagnosed one only two three thing things something stuff loves adores hates likes ice Ice Francies_tossed Melting_polar cream crème creams Maximum_Moisture eat eating ate eaten sugar refined_sugar cane_sugar turbinado free Free Six_Flags_website_http://www.sixflags.com FREE ice Ice Francies_tossed Melting_polar cream crème creams Maximum_Moisture \n"
     ]
    }
   ],
   "source": [
    "#test first question expanded \n",
    "print clean_train_questions[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating the tf/idf...\n",
      "\n",
      "(105L, 900L)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "print(\"Creating the tf/idf...\\n\")\n",
    "# Initialize the \"TfidfVectorizer\" object, which is scikit-learn's tf/idf tool.\n",
    "\"\"\"\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=1, \\\n",
    "                                   max_features=20, \\\n",
    "                                   min_df=1, \\\n",
    "                                   stop_words=None, \\\n",
    "                                   use_idf=True, \\\n",
    "                                   tokenizer=None, \\\n",
    "                                   ngram_range=(1,1)) \n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "CALCULA EL IF-tdf DE ACUERDO A LA EXPRESION \n",
    "morm (tf * LOGN(dT+1/dt+1)+1)\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=900)\n",
    "# Tf-idf-weighted term-document sparse matrix \n",
    "\n",
    "tfidf_train_data_features = tfidf_vectorizer.fit_transform(clean_train_questions)\n",
    "\n",
    "# Convert the result to nampy array \n",
    "\n",
    "tfidf_train_data_features_array= tfidf_train_data_features.toarray() \n",
    "\n",
    "print(tfidf_train_data_features_array.shape)\n",
    "vocab = tfidf_vectorizer.get_feature_names()\n",
    "#print vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.        ,  0.        ,  0.0317715 ,  0.07730862,  0.11417995,\n",
       "        0.066547  ,  0.00398448,  0.00446716,  0.        ,  0.01152385,\n",
       "        0.01205096,  0.        ,  0.02942527,  0.01861577,  0.03034839,\n",
       "        0.0112765 ,  0.02558099,  0.02221115,  0.01471993,  0.01322647,\n",
       "        0.02146023,  0.00722879,  0.0153696 ,  0.        ,  0.03149349,\n",
       "        0.03243212,  0.        ,  0.01382045,  0.01809368,  0.03949409,\n",
       "        0.02707776,  0.04003335,  0.01927105,  0.02581786,  0.        ,\n",
       "        0.        ,  0.01421368,  0.02777035,  0.01582768,  0.        ,\n",
       "        0.05236545,  0.28181401,  0.00791609,  0.03404059,  0.        ,\n",
       "        0.01837193,  0.04532673,  0.        ,  0.08087924,  0.        ,\n",
       "        0.        ,  0.01862787,  0.02413602,  0.05421281,  0.26925085,\n",
       "        0.        ,  0.        ,  0.00949573,  0.        ,  0.        ,\n",
       "        0.02887804,  0.        ,  0.02683942,  0.03122145,  0.02875108,\n",
       "        0.03023074,  0.21367708,  0.        ,  0.01714192,  0.        ,\n",
       "        0.02090076,  0.        ,  0.03914289,  0.        ,  0.        ,\n",
       "        0.05600016,  0.02739045,  0.01785876,  0.        ,  0.01682742,\n",
       "        0.0540346 ,  0.05182375,  0.        ,  0.        ,  0.01609923,\n",
       "        0.00860411,  0.01043696,  0.        ,  0.        ,  0.01720728,\n",
       "        0.11107693,  0.        ,  0.01626562,  0.02118387,  0.23580676,\n",
       "        0.        ,  0.00865084,  0.01039776,  0.        ,  0.01235231,\n",
       "        0.        ,  0.01732398,  0.01040231,  0.        ,  0.        ])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#sanity: check the first question with all content of the tfidf-matrix (all other questions including the first)\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "cosine_similarities = linear_kernel(tfidf_train_data_features[0:1], tfidf_train_data_features).flatten()\n",
    "cosine_similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0, 41, 54, 94], dtype=int64)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "related_indices = cosine_similarities.argsort()[:-5:-1]\n",
    "related_indices\n",
    "#print (train[\"id\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11\n",
      "137\n",
      "152\n",
      "193\n"
     ]
    }
   ],
   "source": [
    "for i in related_indices:\n",
    "    print (train[\"id\"][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.        ,  0.28181401,  0.26925085,  0.23580676])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#it is obvius that the firs question is equal to the first, so the cosine is 1 \n",
    "cosine_similarities[related_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfor i in range(0,10):\\n    cosine_similarities = linear_kernel(tfidf_train_data_features[i:i+1], tfidf_train_data_features).flatten()\\n#cosine_similarities\\n    related_indices = cosine_similarities.argsort()[:-5:-1]\\n    print (\"Question  %d id %d most similar to \\n\" % ( i, train[\"id\"][i])  )  \\n    #related_indices\\n    for i in related_indices:\\n        print (train[\"id\"][i])\\n    #print (train[\"id\"][related_indices])\\n    print\\n    print \"Cosine vector \"\\n    print\\n    print cosine_similarities[related_indices]\\n    print\\n    '"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compute all\n",
    "\"\"\"\n",
    "for i in range(0,10):\n",
    "    cosine_similarities = linear_kernel(tfidf_train_data_features[i:i+1], tfidf_train_data_features).flatten()\n",
    "#cosine_similarities\n",
    "    related_indices = cosine_similarities.argsort()[:-5:-1]\n",
    "    print (\"Question  %d id %d most similar to \\n\" % ( i, train[\"id\"][i])  )  \n",
    "    #related_indices\n",
    "    for i in related_indices:\n",
    "        print (train[\"id\"][i])\n",
    "    #print (train[\"id\"][related_indices])\n",
    "    print\n",
    "    print \"Cosine vector \"\n",
    "    print\n",
    "    print cosine_similarities[related_indices]\n",
    "    print\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question  0 id 11 most similar to \n",
      "\n",
      "137\n",
      "152\n",
      "193\n",
      "165\n",
      "\n",
      "Cosine vector \n",
      "\n",
      "[ 0.28181401  0.26925085  0.23580676  0.21367708]\n",
      "\n",
      "Question  1 id 12 most similar to \n",
      "\n",
      "127\n",
      "203\n",
      "167\n",
      "177\n",
      "\n",
      "Cosine vector \n",
      "\n",
      "[ 0.4655647   0.16698895  0.14794502  0.13020878]\n",
      "\n",
      "Question  2 id 13 most similar to \n",
      "\n",
      "125\n",
      "200\n",
      "204\n",
      "154\n",
      "\n",
      "Cosine vector \n",
      "\n",
      "[ 0.11571468  0.11552708  0.09170331  0.08086858]\n",
      "\n",
      "Question  3 id 14 most similar to \n",
      "\n",
      "103\n",
      "113\n",
      "179\n",
      "164\n",
      "\n",
      "Cosine vector \n",
      "\n",
      "[ 0.17622609  0.10791909  0.10778491  0.1026144 ]\n",
      "\n",
      "Question  4 id 15 most similar to \n",
      "\n",
      "162\n",
      "113\n",
      "149\n",
      "179\n",
      "\n",
      "Cosine vector \n",
      "\n",
      "[ 0.24260646  0.21810148  0.20816221  0.20060492]\n",
      "\n",
      "Question  5 id 16 most similar to \n",
      "\n",
      "192\n",
      "142\n",
      "113\n",
      "183\n",
      "\n",
      "Cosine vector \n",
      "\n",
      "[ 0.19987297  0.15302599  0.1286492   0.12121819]\n",
      "\n",
      "Question  6 id 17 most similar to \n",
      "\n",
      "141\n",
      "194\n",
      "106\n",
      "204\n",
      "\n",
      "Cosine vector \n",
      "\n",
      "[ 0.29029673  0.12869883  0.10625505  0.09289083]\n",
      "\n",
      "Question  7 id 18 most similar to \n",
      "\n",
      "142\n",
      "156\n",
      "151\n",
      "145\n",
      "\n",
      "Cosine vector \n",
      "\n",
      "[ 0.14382795  0.11282771  0.08260155  0.0817832 ]\n",
      "\n",
      "Question  8 id 19 most similar to \n",
      "\n",
      "129\n",
      "146\n",
      "156\n",
      "186\n",
      "\n",
      "Cosine vector \n",
      "\n",
      "[ 0.18643292  0.13556303  0.12057326  0.09807145]\n",
      "\n",
      "Question  9 id 20 most similar to \n",
      "\n",
      "197\n",
      "161\n",
      "202\n",
      "124\n",
      "\n",
      "Cosine vector \n",
      "\n",
      "[ 0.34393225  0.09466746  0.07828402  0.05754486]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#compute all similarities but not including the first 10 text (test questions)\n",
    "for i in range(0,10):\n",
    "    cosine_similarities = linear_kernel(tfidf_train_data_features[i:i+1], tfidf_train_data_features[10:105]).flatten()\n",
    "#cosine_similarities\n",
    "    related_indices = cosine_similarities.argsort()[:-5:-1]\n",
    "    #related_indices+10\n",
    "    print (\"Question  %d id %d most similar to \\n\" % ( i, train[\"id\"][i])  ) \n",
    "    for i in related_indices+10:\n",
    "        print (train[\"id\"][i])\n",
    "    #print (train[\"id\"][related_indices])\n",
    "    print\n",
    "    print \"Cosine vector \"\n",
    "    print\n",
    "    print cosine_similarities[related_indices]\n",
    "    print\n",
    "    #print (train[\"id\"][related_indices+10])\n",
    "    #print cosine_similarities[related_indices]\n",
    "    #print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
